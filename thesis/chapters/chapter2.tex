\chapter{Problemy NP-trudne}

Załóżmy, że jesteśmy menedżerem logistyki, który musi zoptymalizować harmonogram dostaw towarów przez park pojazdów w warunkach ograniczonej sieci dystrybucyjnej obejmującej całe miasto. To pozornie rutynowe zadanie, po bliższym przyjrzeniu się, okazuje się być złożonym problemem optymalizacji kombinatorycznej, charakteryzującym się potrzebą minimalizacji kosztów paliwa, skrócenia czasu podróży i maksymalizacji przepustowości dostaw.

W poszukiwaniu optymalnego rozwiązania mamy do czynienia ze stale rosnącym zestawem zmiennych, w tym nieprzewidywalną dynamiką ruchu, różnymi rozmiarami paczek i dynamicznymi zmianami popytu ze strony klientów. Każda decyzja o ustaleniu konkretnych tras dostawy lub nadaniu priorytetu określonym miejscom docelowym powoduje eksplozję kombinatorycznych możliwości, zmieniając problem optymalizacji logistycznej w przykład problemu NP-trudnego.

Z perspektywy obliczeniowej, problemy NP-trudne są klasą problemów, dla których nie istnieje algorytm wielomianowy, który może zapewnić optymalne rozwiązanie we wszystkich przypadkach. Złożoność ta jest wyraźnie widoczna w problemach takich jak optymalizacja tras, gdzie ogromna przestrzeń rozwiązań nie pozwala na proste rozwiązanie. Złożoność takich problemów logistycznych odzwierciedla szersze trudności występujące w problemach NP-trudnych w różnych obszarach obliczeniowych i optymalizacyjnych.


\section{Teoria złożoności obliczeniowej}

W informatyce teoretycznej i matematyce, teoria złożoności obliczeniowej koncentruje się na klasyfikacji problemów obliczeniowych według ich wykorzystania zasobów i powiązaniu tych klas ze sobą. Problem obliczeniowy to zadanie rozwiązywane przez komputer. Problem obliczeniowy jest rozwiązywalny poprzez mechaniczne zastosowanie kroków matematycznych, takich jak algorytm.

Problem jest uważany za z natury trudny, jeśli jego rozwiązanie wymaga znacznych zasobów, niezależnie od zastosowanego algorytmu. Teoria formalizuje tę intuicję, wprowadzając matematyczne modele obliczeń do badania tych problemów i kwantyfikując ich złożoność obliczeniową, tj. ilość zasobów potrzebnych do ich rozwiązania, takich jak czas i pamięć. Stosowane są również inne miary złożoności, takie jak ilość komunikacji (używana w złożoności komunikacyjnej), liczba bramek w obwodzie (używana w złożoności obwodu) i liczba procesorów (używana w obliczeniach równoległych). Jednym z zadań teorii złożoności obliczeniowej jest określenie praktycznych ograniczeń tego, co komputery mogą, a czego nie mogą zrobić. Problem P versus NP, jeden z siedmiu problemów nagrodzonych Millennium Prize, jest poświęcony dziedzinie złożoności obliczeniowej.

	\subsection{Problemy obliczeniowe}

		\subsubsection{Instancjii problemu}
Problem obliczeniowy może być rozważany jako nieskończony zbiór instancji wraz ze zbiorem (być może pustym) rozwiązań dla każdej instancji. Ciąg wejściowy dla problemu obliczeniowego jest określany jako instancja problemu i nie powinien być mylony z samym problemem. W teorii złożoności obliczeniowej, problem odnosi się do abstrakcyjnego pytania, które należy rozwiązać. W przeciwieństwie do tego, instancja problemu jest raczej konkretną wypowiedzią, która może służyć jako dane wejściowe dla problemu decyzyjnego. Dla przykładu, rozważmy problem testowania pierwotności. Instancją jest liczba (np. 15), a rozwiązaniem jest "tak", jeśli liczba jest pierwsza i "nie" w przeciwnym razie (w tym przypadku 15 nie jest pierwsza i odpowiedź brzmi "nie"). Innymi słowy, instancja jest konkretnym wejściem do problemu, a rozwiązanie jest wyjściem odpowiadającym danemu wejściu.

Aby jeszcze bardziej podkreślić różnicę między problemem a instancją, rozważmy następującą instancję decyzyjnej wersji problemu komiwojażera: Czy istnieje trasa o długości co najwyżej 2000 kilometrów przechodząca przez wszystkie 15 największych miast Niemiec? Ilościowa odpowiedź na ten konkretny przypadek problemu jest mało przydatny do rozwiązywania innych przypadków, takich jak pytanie o podróż w obie strony przez wszystkie dzielnice Mediolanu, których łączna długość wynosi co najwyżej 10 km. Z tego powodu teoria złożoności zajmuje się problemami obliczeniowymi, a nie konkretnymi przypadkami problemów.

		\subsubsection{Reprezentacja instancji problemu}

Rozważając problemy obliczeniowe, instancja problemu jest ciągiem znaków w alfabecie. Zazwyczaj przyjmuje się, że alfabet jest alfabetem binarnym (tj. zbiorem $\{0,1\}$), a zatem ciągi są ciągami bitów. Podobnie jak w prawdziwym komputerze, obiekty matematyczne inne niż ciągi bitów muszą być odpowiednio zakodowane. Na przykład, liczby całkowite mogą być reprezentowane w notacji binarnej, a grafy mogą być kodowane bezpośrednio poprzez ich macierze sąsiedztwa lub poprzez kodowanie ich list sąsiedztwa w zapisie binarnym.

		\subsubsection{Problemy decyzyjne jako języki formalne}

Problem decyzyjny ma tylko dwa możliwe wyjścia, tak lub nie (lub naprzemiennie 1 lub 0) na dowolnym wejściu.
Problemy decyzyjne są jednym z głównych przedmiotów badań w teorii złożoności obliczeniowej. Problem decyzyjny jest specjalnym rodzajem problemu obliczeniowego, którego odpowiedzią jest "tak" lub "nie". Problem decyzyjny może być postrzegany jako język formalny, elementami którego są przypadki z wynikiem "tak", a poza nim są przypadki z wynikiem "nie". Celem jest podjęcie decyzji, za pomocą algorytmu, czy dany ciąg wejściowy jest członkiem rozważanego języka formalnego. Jeśli algorytm rozwiązujący ten problem zwróci odpowiedź twierdzącą, mówi się, że algorytm akceptuje ciąg wejściowy, w przeciwnym razie mówi się, że odrzuca dane wejściowe.

Przykład problemu decyzyjnego jest następujący. Dane wejściowe to dowolny graf. Problem polega na rozstrzygnięciu, czy dany graf jest połączony, czy nie. Formalnym językiem związanym z tym problemem decyzyjnym jest zbiór wszystkich połączonych grafów - aby uzyskać dokładną definicję tego języka, należy zdecydować, w jaki sposób grafy są kodowane jako ciągi binarne.

		\subsubsection{Problemy funkcyjne}

Problem funkcyjny jest problemem obliczeniowym, w którym dla każdego wejścia oczekuje się pojedynczego wyniku (funkcji całkowitej), ale wynik jest bardziej złożony niż w przypadku problemu decyzyjnego - to znaczy, wynik nie jest tylko tak lub nie. Godnymi uwagi przykładami są problem komiwojażera i faktoryzacja liczb całkowitych.

Może się wydawać, że pojęcie problemów funkcyjnych jest znacznie bogatsze niż pojęcie problemów decyzyjnych. Nie jest to jednak prawdą, ponieważ problemy funkcyjne można przekształcić w problemy decyzyjne. Na przykład, mnożenie dwóch liczb całkowitych może być wyrażone jako zbiór trójek $(a, b, c)$ takich, że zachodzi relacja $a × b = c$. Podjęcie decyzji, czy dana trójka jest członkiem tego zbioru, odpowiada rozwiązaniu problemu mnożenia dwóch liczb.

		\subsubsection{Mierzenie rozmiaru instancji}

Aby zmierzyć złożoność obliczeniową problemu, konieczne jest zbadanie czasu wykonania najlepszego algorytmu w zależności od rozmiaru instancji. Jest to istotne ze względu na fakt, że czas wykonania zależy od konkretnego zestawu danych wejściowych, a większe instancje problemu zazwyczaj wymagają dłuższego czasu na rozwiązanie. Czas ten (lub pamięć, lub jakakolwiek miara złożoności) jest tradycyjnie wyrażany jako funkcja rozmiaru instancji, zwykle rozumianego jako liczba bitów reprezentujących dane wejściowe. Teoria złożoności skupia się analizie skalowalności algorytmów wraz ze wzrostem rozmiaru danych wejściowych. Przykładowo, jak znacząco zwiększa się czas rozwiązania problemu dla grafu o 2n wierzchołkach w porównaniu do grafu o n wierzchołkach.

Przyjmując, że rozmiar danych wejściowych wynosi n, czas potrzebny na rozwiązanie może być wyrażony jako funkcja n.  Ponieważ czas rozwiązania różnych zestawów danych o tym samym rozmiarze może się różnić, definiuje się najgorszą złożoność czasową T(n) jako maksymalny czas potrzebny do rozwiązania wszystkich danych wejściowych o rozmiarze n.  Jeśli T(n) jest wielomianem w n, to algorytm jest nazywany algorytmem czasu wielomianowego. Praca Cobhama potwierdza, że problem może być rozwiązany przy użyciu realnej ilości zasobów, jeśli dostępny jest algorytm wielomianowy.

	\subsection{Modele i miary złożoności}
	
	\subsubsection{Maszyna Turinga}
Maszyna Turinga to matematyczny model uniwersalnego komputera. Jest to teoretyczne urządzenie, które manipuluje symbolami zawartymi na taśmie. Maszyny Turinga nie są pomyślane jako praktyczna technologia obliczeniowa, ale raczej jako ogólny model maszyny obliczeniowej - od zaawansowanego superkomputera do matematyka z ołówkiem i kartką papieru. Uważa się, że jeśli jakiś problem może zostać rozwiązany za pomocą algorytmu, to istnieje maszyna Turinga, która rozwiązuje ten problem. Faktycznie, jest to stwierdzenie tezy Churcha-Turinga. Co więcej, wiadomo, że wszystko, co można obliczyć na innych znanych nam obecnie modelach obliczeniowych, takich jak maszyna RAM, Conway's Game of Life, automaty komórkowe, rachunek lambda lub dowolny język programowania, można obliczyć na maszynie Turinga. Ponieważ maszyny Turinga są łatwe do analizy matematycznej i uważa się, że są tak potężne, jak każdy inny model obliczeń, maszyna Turinga jest najczęściej używanym modelem w teorii złożoności.

Wiele typów maszyn Turinga jest używanych do definiowania klas złożoności, takich jak deterministyczne maszyny Turinga, probabilistyczne maszyny Turinga, niedeterministyczne maszyny Turinga, kwantowe maszyny Turinga, symetryczne maszyny Turinga i naprzemienne maszyny Turinga. Zasadniczo wszystkie są równie potężne, ale gdy zasoby (takie jak czas lub przestrzeń) są ograniczone, niektóre z nich mogą być potężniejsze od innych.

Deterministyczna maszyna Turinga jest najbardziej podstawową maszyną Turinga, która wykorzystuje ustalony zestaw reguł do określenia swoich przyszłych akcji. Probabilistyczna maszyna Turinga to deterministyczna maszyna Turinga z dodatkowym zapasem losowych bitów. Zdolność do podejmowania probabilistycznych decyzji często pomaga algorytmom rozwiązywać problemy bardziej efektywnie. Algorytmy wykorzystujące losowe bity nazywane są algorytmami losowymi. Niedeterministyczna maszyna Turinga to deterministyczna maszyna Turinga z dodatkową właściwością niedeterminizmu, która pozwala maszynie Turinga na wiele możliwych akcji w przyszłości z danego stanu. Niedeterminizm można rozumieć w ten sposób, że maszyna Turinga na każdym kroku rozgałęzia się na wiele możliwych ścieżek obliczeniowych, a jeśli rozwiąże problem w którejkolwiek z tych gałęzi, mówi się, że rozwiązała problem. Oczywiście model ten nie jest fizycznie możliwym do zrealizowania - to tylko teoretycznie użyteczna abstrakcyjna maszyna, który pozwala na wyróżnienie wielu klas złożoności. 

\subsubsection{Miary złożoności}
W celu dokładnego zdefiniowania, co oznacza rozwiązanie problemu przy użyciu danej ilości czasu i miejsca, stosuje się model obliczeniowy, taki jak deterministyczna maszyna Turinga. Czas wymagany przez deterministyczną maszynę Turinga M na wejściu x jest całkowitą liczbą przejść stanów lub kroków, które maszyna wykonuje zanim się zatrzyma i wyśle odpowiedź ("tak" lub "nie"). Mówi się, że maszyna Turinga M działa w czasie f(n), jeśli czas wymagany przez M na każdym wejściu o długości n wynosi co najwyżej f(n). Problem decyzyjny A może być rozwiązany w czasie f(n), jeśli istnieje maszyna Turinga działająca w czasie f(n), która rozwiązuje ten problem. Ponieważ teoria złożoności interesuje się klasyfikacją problemów na podstawie ich trudności, definiuje się zbiory problemów na podstawie pewnych kryteriów. Na przykład, zbiór problemów rozwiązywalnych w czasie f(n) na deterministycznej maszynie Turinga jest oznaczany przez DTIME(f(n)).

Analogiczne definicje można stworzyć dla wymagań przestrzennych. Chociaż czas i pamięć są najbardziej znanymi zasobami złożoności, każda miara złożoności może być postrzegana jako zasób obliczeniowy. Miary złożoności są bardzo ogólnie definiowane przez aksjomaty złożoności Bluma. Inne miary złożoności stosowane w teorii złożoności obejmują złożoność komunikacyjną, złożoność obwodu i złożoność drzewa decyzyjnego.

Złożoność algorytmu jest często wyrażana za pomocą notacji duże O.
% sipser

	\subsection{Historia}

Wczesnym przykładem badania złożoności algorytmów jest analiza czasu działania algorytmu Euklidesa przeprowadzona przez Gabriela Lamé w 1844 roku.

Zanim rozpoczęły się rzeczywiste badania wyraźnie poświęcone złożoności problemów algorytmicznych, wielu badaczy opracowało ich podstawy. Najbardziej wpływowym z nich było zdefiniowanie maszyn Turinga przez Alana Turinga w 1936 roku, które okazały się bardzo stabilną i uniwersalną wersją komputera.

Początek systematycznych badań nad złożonością obliczeniową przypisuje się przełomowej pracy z 1965 roku "On the Computational Complexity of Algorithms" autorstwa Jurisa Hartmanisa i Richarda E. Stearnsa, w której określono definicje złożoności czasowej i pamięciowej oraz udowodniono twierdzenia o hierarchii \cite{FortnowH03}. Ponadto, w 1965 roku Edmonds zasugerował, by za "dobry" algorytm uznać taki, którego czas działania jest ograniczony wielomianem rozmiaru danych wejściowych \cite{Karp86}.

Wcześniejsze prace badające problemy rozwiązywalne przez maszyny Turinga z określonymi ograniczonymi zasobami obejmują \cite{FortnowH03} definicję liniowych automatów ograniczonych Johna Myhilla (Myhill 1960), badanie podstawowych zbiorów Raymonda Smullyana (1961), a także artykuł Hisao Yamady \cite{Yamada62a} na temat obliczeń w czasie rzeczywistym (1962). Nieco wcześniej Boris Trakhtenbrot (1956), pionier w tej dziedzinie z ZSRR, badał inną specyficzną miarę złożoności:

"Jednak [moje] początkowe zainteresowanie [teorią automatów] było coraz bardziej odkładane na korzyść złożoności obliczeniowej, ekscytującej fuzji metod kombinatorycznych, odziedziczonych po teorii przełączania, z arsenałem pojęciowym teorii algorytmów. Pomysły te przyszły mi do głowy wcześniej, w 1955 roku, kiedy wymyśliłem termin "funkcja sygnalizacyjna", która obecnie jest powszechnie znana jako "miara złożoności"\cite{Trakhtenbrot08}."

W 1967 roku Manuel Blum sformułował zestaw aksjomatów (obecnie znanych jako aksjomaty Bluma) określających pożądane właściwości miar złożoności na zbiorze funkcji obliczalnych i uzyskał ważny rezultat, tak zwane twierdzenie o przyspieszeniu. Dziedzina ta zaczęła się rozwijać w 1971 roku, kiedy Stephen Cook i Leonid Levin udowodnili istnienie praktycznie ważnych problemów, które są NP-zupełne. W 1972 roku Richard Karp posunął tę ideę o krok naprzód w swoim przełomowym artykule "Reducibility Among Combinatorial Problems", w którym wykazał, że 21 różnych problemów kombinatorycznych i grafowych, z których każdy znany jest z niewykonalności obliczeniowej, są NP-zupełne \cite{Karp86}.

\section{Definicja klasy problemów NP-trudnych}

\subsection{Klasy złożoności}
Klasa złożoności to zbiór problemów o zbliżonej złożoności obliczeniowej. 

\subsection{Redukcja}

\subsection{Klasa NP-hard}


%\begin{lstlisting}
%\end{lstlisting}